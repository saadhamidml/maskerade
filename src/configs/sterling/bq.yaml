# seed should ideally be in the experiment dictionary, but keeping it at
# the root allows Sacred to manage it.
#seed: 0
experiment:
  directories:
    data: "../data"
    logs: "../logs"
  smoke_test: False
  disable_cuda: False
  cuda_device: 0
  default_data_type: "float64"
problem:
  name: "sterling"
  test_size: 0.1
  shuffle: True
model:
  type: "gaussian_process"
  collection: True
  approximation: "exact"
  mean_function:
    type: "constant"
  covariance_function:
    type: "keops_bayesian_gaussian_spectral_mixture"
    num_mixtures: [1, 2, 3, 4, 5]
    prior:
      component_cardinality:
        type: "uniform"
      priors:
        - weights:
            type: "dirichlet"
            parameters:
              concentration: [1]
          means:
            type: "uniform"
            parameters:
              lower: [0]
              upper: [250]
          scales:
            type: "lognormal"
            parameters:
              loc: [3.0]
              scale: [0.69]
        - weights:
            type: "dirichlet"
            parameters:
              concentration: [1, 1]
          means:
            type: "uniform"
            parameters:
              lower: [0, 0]
              upper: [250, 250]
          scales:
            type: "lognormal"
            parameters:
              loc: [3.0, 3.0]
              scale: [0.69, 0.69]
        - weights:
            type: "dirichlet"
            parameters:
              concentration: [1, 1, 1]
          means:
            type: "uniform"
            parameters:
              lower: [0, 0, 0]
              upper: [250, 250, 250]
          scales:
            type: "lognormal"
            parameters:
              loc: [3.0, 3.0, 3.0]
              scale: [0.69, 0.69, 0.69]
        - weights:
            type: "dirichlet"
            parameters:
              concentration: [1, 1, 1, 1]
          means:
            type: "uniform"
            parameters:
              lower: [0, 0, 0, 0]
              upper: [250, 250, 250, 250]
          scales:
            type: "lognormal"
            parameters:
              loc: [3.0, 3.0, 3.0, 3.0]
              scale: [0.69, 0.69, 0.69, 0.69]
        - weights:
            type: "dirichlet"
            parameters:
              concentration: [1, 1, 1, 1, 1]
          means:
            type: "uniform"
            parameters:
              lower: [0, 0, 0, 0, 0]
              upper: [250, 250, 250, 250, 250]
          scales:
            type: "lognormal"
            parameters:
              loc: [3.0, 3.0, 3.0, 3.0, 3.0]
              scale: [0.69, 0.69, 0.69, 0.69, 0.69]
  likelihood:
    type: "gaussian"
  marginal_log_likelihood:
    type: "exact"
numerics:
  strategy: "bayesian_quadrature"
  load_id: null
  evaluation_budget: 99999
  time_budget: 1200
  infer_while_learning: False
  initialisation:
    strategy: "sample_prior"
    num_samples: 100
  surrogate:
    model:
      warping: "linearised_square_root"
      mean_function:
        type: "constant"
      covariance_function:
        type: "energy_distance_mmd"
      likelihood:
        type: "gaussian"
      marginal_log_likelihood:
        type: "exact"
      alpha_factor: 0.8
    numerics:
      strategy: "optimisation"
      initialisation:
        lengthscale: 10.0
      optimiser:
        type: "lbfgs"
        learning_rate: 0.1
        num_iterations: 100
      optimise_every: 2
  acquisition_function:
    type: "fit_bq"
    numerics:
      strategy: "optimisation"
      initialisation:
        strategy: "sample_prior"
        num_samples: 1000
      optimiser:
        type: "lbfgs"
        learning_rate: 0.1
    batch_size: 20
    num_batches: 1
  kernel_integration_num_samples: 1000